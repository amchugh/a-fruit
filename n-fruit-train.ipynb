{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Get all the imports\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Flatten, LeakyReLU, BatchNormalization, Reshape\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.engine.input_layer import Input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup some variables\n",
    "image_rows = 52\n",
    "image_cols = 52\n",
    "channels = 3\n",
    "BS=64\n",
    "generated_image_path = \"generated_images/\"\n",
    "image_shape = (image_rows, image_cols, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(im):\n",
    "    return (im.astype(np.float32) - 127.5)/127.5\n",
    "\n",
    "def reformat_image(im):\n",
    "    return ((im * 127.5) + 127.5).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class exists as to keep the entire array of\n",
    "# images out of RAM and instead load them in as needed\n",
    "class ImageLoader():\n",
    "    def __init__(self, files, func):\n",
    "        self.files = files\n",
    "        self.c_index = 0\n",
    "        self.max = len(self.files)\n",
    "        self.func = func\n",
    "    \n",
    "    def getNextFiles(self, num):\n",
    "        arr = []\n",
    "        for i in range(num):\n",
    "            arr.append(self.load_image(self.files[self.c_index]))\n",
    "            self.c_index += 1\n",
    "            if self.c_index == self.max:\n",
    "                print(\"Looping data now\")\n",
    "                self.c_index = 0\n",
    "        return self.func(np.array(arr))\n",
    "        \n",
    "    def load_image(self, filename):\n",
    "        img = Image.open(filename)\n",
    "        img = img.resize((image_rows,image_cols))\n",
    "        img = list(img.getdata())\n",
    "        img = np.array(img)\n",
    "        return img.reshape(image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NFruit():\n",
    "    def __init__(self):\n",
    "        self.discriminator = self.makeDiscriminator()\n",
    "        self.generator = self.makeGenerator()\n",
    "        \n",
    "        # Compile the models\n",
    "        #optimizer = SGD(0.00002,momentum=0.3, decay=0.000001, nesterov=True)\n",
    "        optimizer = Adam(0.002,0.5);\n",
    "        self.discriminator.compile(\n",
    "            loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "        self.generator.compile(\n",
    "            loss='binary_crossentropy',\n",
    "            optimizer=optimizer)\n",
    "        \n",
    "        # Build the combined model\n",
    "        model_input = Input(shape=(80,))\n",
    "        image = self.generator(model_input)\n",
    "        #self.dm.trainable = False\n",
    "        validifier = self.discriminator(image)\n",
    "        self.combined = Model(model_input, validifier)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "        self.combined.summary()\n",
    "        \n",
    "        # Create the image loader and the give it the collection of images\n",
    "        image_locs = glob(\"fruits/fruits-360/Training/*/*.jpg\")\n",
    "        self.il = ImageLoader(image_locs, preprocess_input)\n",
    "        \n",
    "    def makeDiscriminator(self):\n",
    "        # Create our discriminator.\n",
    "        # This is the model that attempts to find\n",
    "        # real and fake fruit\n",
    "        dm = Sequential()\n",
    "        dm.add(Flatten(input_shape=image_shape))\n",
    "        #dm.add(Dense(1024))\n",
    "        #dm.add(LeakyReLU(alpha=0.25))\n",
    "        dm.add(Dense(512))\n",
    "        dm.add(LeakyReLU(alpha=0.25))\n",
    "        dm.add(Dense(256))\n",
    "        dm.add(LeakyReLU(alpha=0.25))\n",
    "        dm.add(Dense(1, activation='sigmoid'))\n",
    "        dm.summary()\n",
    "        img = Input(shape=image_shape)\n",
    "        validity = dm(img)\n",
    "        dm = Model(img, validity)\n",
    "        return dm\n",
    "    \n",
    "    def makeGenerator(self):\n",
    "        # Create our generator\n",
    "        # This is the model that attempts\n",
    "        # to fool the discriminator\n",
    "        noise_shape = (80,)\n",
    "        gn = Sequential()\n",
    "        gn.add(Dense(256, input_shape=noise_shape))\n",
    "        gn.add(LeakyReLU(alpha=0.25))\n",
    "        gn.add(BatchNormalization(momentum=0.9))\n",
    "        gn.add(Dense(512))\n",
    "        gn.add(LeakyReLU(alpha=0.25))\n",
    "        gn.add(BatchNormalization(momentum=0.9))\n",
    "        #gn.add(Dense(1024))\n",
    "        #gn.add(LeakyReLU(alpha=0.25))\n",
    "        #gn.add(BatchNormalization(momentum=0.9))\n",
    "        gn.add(Dense(np.prod(image_shape), activation='sigmoid'))\n",
    "        gn.add(Reshape(image_shape))\n",
    "        gn.summary()\n",
    "        n = Input(shape=noise_shape)\n",
    "        img = gn(n)\n",
    "        gn = Model(n, img)\n",
    "        return gn\n",
    "    \n",
    "    def generate_image(self, count=1):\n",
    "        noise = np.random.normal(0,1,(count,80))\n",
    "        return self.generator.predict(noise)\n",
    "    \n",
    "    # Write a function to save some images to files\n",
    "    def save_image_array(self, filename, file_shape):\n",
    "        r, c = file_shape\n",
    "        images = self.generate_image(count=np.prod(file_shape))\n",
    "        f, a = plt.subplots(r,c)\n",
    "        for x in range(r):\n",
    "            for y in range(c):\n",
    "                a[x,y].imshow(images[x*c+y,:,:,:])\n",
    "                a[x,y].axis('off')\n",
    "        f.savefig(filename)\n",
    "        plt.close()\n",
    "                  \n",
    "    def train(self, epochs = 10000, print_interval = 10):\n",
    "        dm_total_hist = []\n",
    "        self.past_data = []\n",
    "        print(\"Starting\")\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            # train the discriminator on the batch of real data\n",
    "            real_data = self.il.getNextFiles(BS//2)\n",
    "            self.past_data.append(real_data)\n",
    "            dm_hist_real = self.discriminator.train_on_batch(real_data, np.ones((BS//2, 1))\n",
    "                                            )\n",
    "            # train the discriminator on the batch of fake data\n",
    "            #fake_data = generate_image(count=BS//2)\n",
    "            noise = np.random.normal(0, 1, (BS//2, 80))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "            \n",
    "            dm_hist_fake = self.discriminator.train_on_batch(real_data, np.zeros((BS//2, 1)))\n",
    "            \n",
    "            # calculate total loss\n",
    "            dm_loss = np.add(dm_hist_real, dm_hist_fake) / 2\n",
    "            dm_total_hist.append(dm_loss)\n",
    "            \n",
    "            # Now train the generator\n",
    "            noise = np.random.normal(0,1,(BS,80))\n",
    "            valid_y = np.array([1] * BS)\n",
    "            gn_loss = self.combined.train_on_batch(noise, valid_y)\n",
    "            \n",
    "            \n",
    "            if e % print_interval == 0:\n",
    "                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (e, dm_loss[0], 100*dm_loss[1], gn_loss))\n",
    "                self.save_image_array(generated_image_path + \n",
    "                                 \"generated_images\" + str(e) + \n",
    "                                 \".png\", (5,5))\n",
    "                \n",
    "        return dm_total_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 8112)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               4153856   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 4,285,441\n",
      "Trainable params: 4,285,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               20736     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8112)              4161456   \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 52, 52, 3)         0         \n",
      "=================================================================\n",
      "Total params: 4,316,848\n",
      "Trainable params: 4,315,312\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 52, 52, 3)         4316848   \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 1)                 4285441   \n",
      "=================================================================\n",
      "Total params: 8,602,289\n",
      "Trainable params: 8,600,753\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nfruit = NFruit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "0 [D loss: 8.167011, acc.: 50.00%] [G loss: 16.118095]\n",
      "1 [D loss: 7.971192, acc.: 50.00%] [G loss: 16.118095]\n",
      "2 [D loss: 7.971192, acc.: 50.00%] [G loss: 16.118095]\n",
      "3 [D loss: 7.971192, acc.: 50.00%] [G loss: 16.118095]\n",
      "4 [D loss: 7.971192, acc.: 50.00%] [G loss: 16.118095]\n",
      "5 [D loss: 7.971192, acc.: 50.00%] [G loss: 16.118095]\n",
      "6 [D loss: 7.971192, acc.: 50.00%] [G loss: 16.118095]\n",
      "7 [D loss: 7.971192, acc.: 50.00%] [G loss: 16.118095]\n",
      "8 [D loss: 7.971192, acc.: 50.00%] [G loss: 16.118095]\n",
      "9 [D loss: 7.971192, acc.: 50.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([8.167011, 0.5     ], dtype=float32),\n",
       " array([7.9711924, 0.5      ], dtype=float32),\n",
       " array([7.9711924, 0.5      ], dtype=float32),\n",
       " array([7.9711924, 0.5      ], dtype=float32),\n",
       " array([7.9711924, 0.5      ], dtype=float32),\n",
       " array([7.9711924, 0.5      ], dtype=float32),\n",
       " array([7.9711924, 0.5      ], dtype=float32),\n",
       " array([7.9711924, 0.5      ], dtype=float32),\n",
       " array([7.9711924, 0.5      ], dtype=float32),\n",
       " array([7.9711924, 0.5      ], dtype=float32)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfruit.train(epochs=10,print_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfruit.save_image_array(generated_image_path + \"generated_images.png\", (5,5))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write a function to generate fake images from our generator\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nfruit.past_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(nfruit.past_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(arr[0],arr[1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "images = il.getNext(BS//2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class NFruit2():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.image_rows = 28\n",
    "        self.image_cols = 28\n",
    "        self.channels = 3\n",
    "        self.BS=64\n",
    "        self.generated_image_path = \"generated_images/\"\n",
    "        self.image_shape = (image_rows, image_cols, channels)\n",
    "        \n",
    "        self.discriminator = self.makeDiscriminator()\n",
    "        self.generator = self.makeGenerator()\n",
    "        \n",
    "        # Compile the models\n",
    "        optimizer = Adadelta(0.0002, 0.5)\n",
    "        self.discriminator.compile(\n",
    "            loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "        self.generator.compile(\n",
    "            loss='binary_crossentropy',\n",
    "            optimizer=optimizer)\n",
    "        \n",
    "        # Build the combined model\n",
    "        model_input = Input(shape=(100,))\n",
    "        image = self.generator(model_input)\n",
    "        #self.dm.trainable = False\n",
    "        validifier = self.discriminator(image)\n",
    "        self.combined = Model(model_input, validifier)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "        self.combined.summary()\n",
    "        \n",
    "        # Create the image loader and the give it the collection of images\n",
    "        image_locs = glob(\"fruits/fruits-360/Training/*/*.jpg\")\n",
    "        self.il = ImageLoader(image_locs)\n",
    "        \n",
    "    def makeDiscriminator(self):\n",
    "        # Create our discriminator.\n",
    "        # This is the model that attempts to find\n",
    "        # real and fake fruit\n",
    "        dm = Sequential()\n",
    "        \n",
    "        dm.add(Flatten(input_shape=image_shape))\n",
    "        dm.add(Dense(512))\n",
    "        dm.add(LeakyReLU(alpha=0.2))\n",
    "        dm.add(Dense(256))\n",
    "        dm.add(LeakyReLU(alpha=0.2))\n",
    "        dm.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        dm.summary()\n",
    "        \n",
    "        img = Input(shape=image_shape)\n",
    "        validity = dm(img)\n",
    "        \n",
    "        return Model(img, validity)\n",
    "    \n",
    "    def makeGenerator(self):\n",
    "        # Create our generator\n",
    "        # This is the model that attempts\n",
    "        # to fool the discriminator\n",
    "        noise_shape = (100,)\n",
    "        gn = Sequential()\n",
    "        \n",
    "        gn.add(Dense(256, input_shape=noise_shape))\n",
    "        gn.add(LeakyReLU(alpha=0.25))\n",
    "        gn.add(BatchNormalization(momentum=0.8))\n",
    "        gn.add(Dense(512))\n",
    "        gn.add(LeakyReLU(alpha=0.25))\n",
    "        gn.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        gn.add(Dense(np.prod(self.image_shape), activation='tanh'))\n",
    "        gn.add(Reshape(image_shape))\n",
    "        gn.summary()\n",
    "        n = Input(shape=noise_shape)\n",
    "        img = gn(n)\n",
    "        gn = Model(n, img)\n",
    "        return gn\n",
    "    \n",
    "    def generate_image(self, count=1):\n",
    "        noise = np.random.normal(0,1,(count,80))\n",
    "        return self.generator.predict(noise)\n",
    "    \n",
    "    # Write a function to save some images to files\n",
    "    def save_image_array(self, filename, file_shape):\n",
    "        r, c = file_shape\n",
    "        images = self.generate_image(count=np.prod(file_shape))\n",
    "        f, a = plt.subplots(r,c)\n",
    "        for x in range(r):\n",
    "            for y in range(c):\n",
    "                a[x,y].imshow(images[x*c+y,:,:,:])\n",
    "                a[x,y].axis('off')\n",
    "        f.savefig(filename)\n",
    "        plt.close()\n",
    "        \n",
    "   \n",
    "    def train(self):\n",
    "        dm_total_hist = []\n",
    "        self.past_data = []\n",
    "        print_interval = 10\n",
    "        print(\"Starting\")\n",
    "        \n",
    "        for e in range(1000):\n",
    "            # train the discriminator on the batch of real data\n",
    "            real_data = self.il.getNextFiles(BS//2)\n",
    "            self.past_data.append(real_data)\n",
    "            dm_hist_real = self.discriminator.train_on_batch(real_data, np.ones((BS//2, 1))\n",
    "                                            )\n",
    "            # train the discriminator on the batch of fake data\n",
    "            #fake_data = generate_image(count=BS//2)\n",
    "            noise = np.random.normal(0, 1, (BS//2, 80))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "            \n",
    "            dm_hist_fake = self.discriminator.train_on_batch(real_data, np.zeros((BS//2, 1)))\n",
    "            \n",
    "            # calculate total loss\n",
    "            dm_loss = np.add(dm_hist_real, dm_hist_fake) / 2\n",
    "            dm_total_hist.append(dm_loss)\n",
    "            \n",
    "            # Now train the generator\n",
    "            noise = np.random.normal(0,1,(BS,80))\n",
    "            valid_y = np.array([1] * BS)\n",
    "            gn_loss = self.combined.train_on_batch(noise, valid_y)\n",
    "            \n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (e, dm_loss[0], 100*dm_loss[1], gn_loss))\n",
    "            \n",
    "            if e % print_interval == 0:\n",
    "                print(\"Finished with %s\" % (e))\n",
    "                self.save_image_array(generated_image_path + \n",
    "                                 \"generated_images\" + str(e) + \n",
    "                                 \".png\", (5,5))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# train the discriminator on the batch of real data\n",
    "real_data = il.getNext(BS//2)\n",
    "dm_hist_real = dm.train_on_batch(real_data, np.ones((BS//2, 1)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# train the discriminator on the batch of fake data\n",
    "fake_data = generate_image(count=BS//2)\n",
    "dm_hist_fake = dm.train_on_batch(fake_data, np.zeros((BS//2, 1)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dm_loss = np.add(dm_hist_real, dm_hist_fake) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_total_hist[:5] + dm_total_hist[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image_array(generated_image_path + \"generated_images.png\", (5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_locs = glob(\"fruits/fruits-360/Training/*/*.jpg\")\n",
    "il = ImageLoader(image_locs, preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = (5,5)\n",
    "images = il.getNextFiles(r*c)\n",
    "f, a = plt.subplots(r,c)\n",
    "for x in range(r):\n",
    "    for y in range(c):\n",
    "        a[x,y].imshow(images[x*c+y,:,:,:])\n",
    "        a[x,y].axis('off')\n",
    "plt.show()\n",
    "plt.close()\n",
    "images = il.getNextFiles(r*c)\n",
    "f, a = plt.subplots(r,c)\n",
    "for x in range(r):\n",
    "    for y in range(c):\n",
    "        a[x,y].imshow(images[x*c+y,:,:,:])\n",
    "        a[x,y].axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
